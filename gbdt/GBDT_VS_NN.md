
# 问为什么很多领域的比赛数据集上的GBDT效果那么好（比神经网络）

## 非线性

人的解决问题，大概就是把一个复杂目标化简解决的能力，目的是降低问题的非线性，把一个问题去解决的过程。具体到数据挖掘上，应该是把特征表达出来的能力。

具体到问题，非线性比较高的场景有，序列建模，大规模离散ID建模，阴阳话识别，语音的特征表达。这些都是非线性非常高的场景，基本都超出了手动解决输入到目标之间gap的能力。

简而言之当你的认知超越问题的难度时候，可以通过一些方法把问题去复杂化，一般GBDT的效果会比NN好。


## kaggle上的数据和赛题有什么特点？

跟现在研究生入学，大部分用深度学习对图片文本不一样，以前kaggle赛题，尤其是2019年前，有很大比例是工业界的表格数据。比如各种实际的预测预估任务，CTR，信用评分，销量预测等。他们有如下几个特点。

1.工业界的数据脏。异常点，缺失值，历史遗留问题造成的数据痕迹等等。

2.工业界的数据可解释性很强，每一列有真实的业务含义。

## GBDT和NN有什么特点

- GBDT

优势：

1.鲁棒，异常点，缺失值都是可以学习的信息

2.适中的非线性能力，在一定范围内是优势

3.可解释性很好，可以帮你优化特征工程

劣势：

1.非线性表达能力有限，很难在文本图像上有用。

2.数据量带来的边际增益不大，容易触及天花板。

- NN

优势：

1.全自动化的特征工程

2.模型容量大，可以利用数据量的优势

劣势：

1.异常值敏感，依赖手动处理

2.不可解释，改进迭代的过程有点像蒙特卡洛，措施和结果的路径太远。

3.过强的非线性中隐含过拟合和噪音。

## 但是看起来LGB的优点在其他模型也有，那为什么不是SVM和LR？

1.这两种模型获取非线性的方式太粗暴了，有种大炮打蚊子的感觉。依靠kernel强行把VC维提高，带来的噪声特别多，有用信息很少，并且kernal是有先验的，很容易被人设的参数带跑偏。这在实际业务数据中是非常致命的。

2.理论上LR+完美的特征工程可以很强，但是太难了，又不是人人都是特征工程大师。早期凤巢亿级特征跑LR效果特别好逐渐成为传说。


## 怎么根据数据特点进行模型选型

- 数据量大小

- 数据到预测目标的非线性

- 单列数据可解释性

- 特征工程天花板高低


XGB/LGB/CTB在最后两个上很有优势。NN在前两个方面很有优势。


















# reference

[为什么GBDT可以超越深度学习](https://blog.51cto.com/u_15671528/5561324)

